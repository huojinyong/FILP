{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_dates参数：\n",
    "# 将csv中的时间字符串转换成日期格式\n",
    "# 地点在建筑物还是十字路口对预测犯罪类型有助"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.geometry import  Point\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import urllib.request\n",
    "import shutil\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import contextily as ctx\n",
    "import geoplot as gplt\n",
    "import lightgbm as lgb\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from lightgbm import LGBMClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\n",
    "#对Dates列名对应的时间转换格式   parse_dates参数： 将csv中的时间字符串转换成日期格式 \n",
    "test = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n",
    "#指定 Id 列作为索引\n",
    "# 在index_col指定表格中的第几列作为Index时需要小心。如本例中，指定参数index_col=0，\n",
    "# 则此时会以新生成的time_date列而不是name作为Index。因此保险的方法是指定列名，如index_col = 'name'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First date: ', str(train.Dates.describe()['first']))\n",
    "print('Last date: ', str(train.Dates.describe()['last']))\n",
    "print('Test data shape ', train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes  #观察各属性 数据类型\n",
    "#The dataset contains a lot of 'object' variables (aka strings) that we will need to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.duplicated().sum()\n",
    "#它还包含2323个重复项，我们应将其删除。\n",
    "#我们还将使用坐标评估数据点的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detailed TODO\n",
    "#画出图像，以地理图为画布，DataFrame画出犯罪地点的经纬度\n",
    "def create_gdf(df):\n",
    "    gdf = df.copy()\n",
    "    gdf['Coordinates'] = list(zip(gdf.X, gdf.Y))\n",
    "    gdf.Coordinates = gdf.Coordinates.apply(Point)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})\n",
    "    return gdf\n",
    "\n",
    "train_gdf = create_gdf(train)\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "ax = world.plot(color='white', edgecolor='black')\n",
    "train_gdf.plot(ax=ax, color='red')\n",
    "plt.show()\n",
    "#图像显示海区是犯罪地点说明这些数据是错误无用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_gdf.loc[train_gdf.Y > 50].count()[0])\n",
    "train_gdf.loc[train_gdf.Y > 50].sample(5)\n",
    "#错误数据输出：67个\n",
    "#取五个样本瞧一瞧\n",
    "#发现 经纬度对应120 和 90 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#从异常值和重复项中清除数据集\n",
    "train.drop_duplicates(inplace=True)\n",
    "#把经纬度对应120 和 90 的数据清除\n",
    "train.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "test.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "#inplace=True就把{'X': -120.5, 'Y': 90.0}替换成np.NaN\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "#数据缺失值补全方法 sklearn.impute.SimpleImputer\n",
    "#默认属性 missing_values=np.nan  \n",
    "\n",
    "#imp.fit_transform(df) 填充\n",
    "for district in train['PdDistrict'].unique():\n",
    "    #train['PdDistrict'] == district 返回相应的索引列表\n",
    "    #e.g. train.loc[['Id1','Id2']]--这些索引对应的dataframe\n",
    "        train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n",
    "            train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n",
    "        test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n",
    "        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n",
    "##Detailed TODO  imp.fit_transform\n",
    "train_gdf = create_gdf(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们检查变量\n",
    "#画图观察各属性中哪些是对预测有用的\n",
    "#Dates Hours\n",
    "#日期和星期几\n",
    "# 这些变量在2003年1月1日至2015年5月13日（以及周一至周日）之间均匀分布，并在训练和测试数据集之间进行分配（如前所述）。我们没有注意到这些变量有任何异常。\n",
    "# 事故的中位数频率为每天389次，标准偏差为48.51。\n",
    "\n",
    "#画出事件每天发生几起的分布---发现像是个正态分布，平平无奇\n",
    "#Detailed TODO \n",
    "col = sns.color_palette()\n",
    "\n",
    "train['Date'] = train.Dates.dt.date\n",
    "train['Hour'] = train.Dates.dt.hour\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "data = train.groupby('Date').count().iloc[:, 0]\n",
    "sns.kdeplot(data=data, shade=True)\n",
    "plt.axvline(x=data.median(), ymax=0.95, linestyle='--', color=col[1])\n",
    "plt.annotate(\n",
    "    'Median: ' + str(data.median()),\n",
    "    xy=(data.median(), 0.004),\n",
    "    xytext=(200, 0.005),\n",
    "    arrowprops=dict(arrowstyle='->', color=col[1], shrinkB=10))\n",
    "plt.title(\n",
    "    'Distribution of number of incidents per day', fontdict={'fontsize': 16})\n",
    "plt.xlabel('Incidents')\n",
    "plt.ylabel('Density')\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#同样，整个星期的事件发生频率也没有明显的偏差。因此，我们预计该变量不会在预测中发挥重要作用。\n",
    "#画出事件相对于星期几的直方图，发现分布均匀\n",
    "#Detailed TODO \n",
    "data = data.reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "    'Sunday'\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = sns.barplot(\n",
    "        data.index, (data.values / data.values.sum()) * 100,\n",
    "        orient='v',\n",
    "        palette=cm.ScalarMappable(cmap='Reds').to_rgba(data.values))\n",
    "\n",
    "plt.title('Incidents per Weekday', fontdict={'fontsize': 16})\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Incidents (%)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别\n",
    "#警察局共记录了39个离散类别的事件，最常见的是盗窃/盗窃（19.91％），非/犯罪（10.50％）和殴打（8.77％）。\n",
    "#竖轴对应类别---画出直方图，横轴是发生率\n",
    "data = train.groupby('Category').count().iloc[:, 0].sort_values(\n",
    "    ascending=False)\n",
    "data = data.reindex(np.append(np.delete(data.index, 1), 'OTHER OFFENSES'))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = sns.barplot(\n",
    "        (data.values / data.values.sum()) * 100,\n",
    "        data.index,\n",
    "        orient='h',\n",
    "        palette=\"Reds_r\")\n",
    "\n",
    "plt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\n",
    "plt.xlabel('Incidents (%)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 警察区\n",
    "# 城市的不同地区之间存在显着差异，其中南部地区的事件发生率最高（17.87％），其次是Mission（13.67％）和Northern（12.00％）。\n",
    "# Downloading the shapefile of the area \n",
    "#以洛杉矶地理图为背景画出热图，越红犯罪率越高\n",
    "url = 'https://data.sfgov.org/api/geospatial/wkhw-cjsf?method=export&format=Shapefile'\n",
    "with urllib.request.urlopen(url) as response, open('pd_data.zip', 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "# Unzipping it\n",
    "with zipfile.ZipFile('pd_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('pd_data')\n",
    "# Loading to a geopandas dataframe\n",
    "for filename in os.listdir('./pd_data/'):\n",
    "    if re.match(\".+\\.shp\", filename):\n",
    "        pd_districts = gpd.read_file('./pd_data/'+filename)\n",
    "        break\n",
    "# Defining the coordinate system to longitude/latitude\n",
    "pd_districts.crs={'init': 'epsg:4326'}\n",
    "\n",
    "# Merging our train dataset with the geo-dataframe\n",
    "pd_districts = pd_districts.merge(\n",
    "    train.groupby('PdDistrict').count().iloc[:, [0]].rename(\n",
    "        columns={'Dates': 'Incidents'}),\n",
    "    how='inner',\n",
    "    left_on='district',\n",
    "    right_index=True,\n",
    "    suffixes=('_x', '_y'))\n",
    "\n",
    "# Transforming the coordinate system to Spherical Mercator for\n",
    "# compatibility with the tiling background\n",
    "pd_districts = pd_districts.to_crs({'init': 'epsg:3857'})\n",
    "\n",
    "# Calculating the incidents per day for every district\n",
    "train_days = train.groupby('Date').count().shape[0]\n",
    "pd_districts['inc_per_day'] = pd_districts.Incidents/train_days\n",
    "\n",
    "# Ploting the data\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "pd_districts.plot(\n",
    "    column='inc_per_day',\n",
    "    cmap='Reds',\n",
    "    alpha=0.6,\n",
    "    edgecolor='r',\n",
    "    linestyle='-',\n",
    "    linewidth=1,\n",
    "    legend=True,\n",
    "    ax=ax)\n",
    "\n",
    "def add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n",
    "    \"\"\"Function that add the tile background to the map\"\"\"\n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n",
    "    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n",
    "    # restore original x/y limits\n",
    "    ax.axis((xmin, xmax, ymin, ymax))\n",
    "\n",
    "# Adding the background\n",
    "add_basemap(ax, zoom=11, url=ctx.sources.ST_TONER_LITE)\n",
    "\n",
    "# Adding the name of the districts\n",
    "for index in pd_districts.index:\n",
    "    plt.annotate(\n",
    "        pd_districts.loc[index].district,\n",
    "        (pd_districts.loc[index].geometry.centroid.x,\n",
    "         pd_districts.loc[index].geometry.centroid.y),\n",
    "        color='#353535',\n",
    "        fontsize='large',\n",
    "        fontweight='heavy',\n",
    "        horizontalalignment='center'\n",
    "    )\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 地址\n",
    "# 地址，作为文本字段，需要先进的技术才能将其用于预测。\n",
    "#取而代之的是，在此项目中，我们将提取它来判断事故是否发生在道路上或建筑构件中。\n",
    "# X-经度Y-纬度\n",
    "# 我们已经测试了坐标在城市边界内。尽管经度不包含任何异常值，但纬度包含一些与北极相对应的90o值。\n",
    "# 探索性可视化\n",
    "# 根据项目的声明，我们需要根据时间和地点来预测每种犯罪的可能性。话虽如此，我们提供了两个图表来可视化这些变量的重要性。\n",
    "#第一个介绍了9个随机犯罪类别的地理密度。我们可以看到，尽管大多数犯罪的重心位于城市的东北部，但每种犯罪在城市其他地方的密度却不同。这是一个可靠的迹象，表明位置（坐标/警区）将成为分析和预测的重要因素。\n",
    "crimes = train['Category'].unique().tolist()\n",
    "crimes.remove('TREA')\n",
    "\n",
    "pd_districts = pd_districts.to_crs({'init':'epsg:4326'})\n",
    "sf_land = pd_districts.unary_union\n",
    "sf_land = gpd.GeoDataFrame(gpd.GeoSeries(sf_land), crs={'init':'epsg:4326'})\n",
    "sf_land = sf_land.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(12,12))\n",
    "for i , crime in enumerate(np.random.choice(crimes, size=9, replace=False)):\n",
    "    data = train_gdf.loc[train_gdf['Category'] == crime]\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    gplt.kdeplot(data,\n",
    "                 shade=True,\n",
    "                 shade_lowest=False,\n",
    "                 clip = sf_land.geometry,\n",
    "                 cmap='Reds',\n",
    "                 ax=ax)\n",
    "    gplt.polyplot(sf_land, ax=ax)\n",
    "    ax.set_title(crime) \n",
    "plt.suptitle('Geographic Density of Different Crimes')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二张图显示了五种犯罪类别每小时的平均事件数。\n",
    "# 显然，不同的犯罪在一天中的不同时间发生的频率不同。\n",
    "# 例如，卖淫在傍晚和整个晚上都有，赌博事件从深夜开始直到早晨，而盗窃案则从清晨到下午进行。\n",
    "# 像以前一样，这些都是清晰的证据，表明时间参数也将发挥重要作用。\n",
    "# 画图横轴为0-24h 为不同犯罪类型做曲线，竖轴为犯罪数\n",
    "data = train.groupby(['Hour', 'Date', 'Category'],\n",
    "                     as_index=False).count().iloc[:, :4]\n",
    "data.rename(columns={'Dates': 'Incidents'}, inplace=True)\n",
    "data = data.groupby(['Hour', 'Category'], as_index=False).mean()\n",
    "data = data.loc[data['Category'].isin(\n",
    "    ['ROBBERY', 'GAMBLING', 'BURGLARY', 'ARSON', 'PROSTITUTION'])]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax = sns.lineplot(x='Hour', y='Incidents', data=data, hue='Category')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\n",
    "plt.suptitle('Average number of incidents per hour')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算法与技术\n",
    "# 特定问题是典型的多类分类问题，有几种算法可以解决该问题。\n",
    "# 最初，我们使用线性模型（随机梯度下降），最近邻（K个最近邻），集成方法（随机森林和AdaBoost）和增强算法（XGBoost和LIghtGBM）中的几种合适算法进行了评估，\n",
    "# 并使用默认参数进行评估是否其中任何一个有重大的领先优势\n",
    "# 其实SGD效果最好\n",
    "#我们决定使用LightGBM，因为它具有超参数调整的效率和多功能性。 \n",
    "#LightGBM是一种决策树增强算法，它使用基于直方图的算法，该算法将连续特征（属性）值存储到离散的bin中。此技术可加快训练速度并减少内存使用量。用外行术语来说，该算法的工作方式如下：\n",
    "# 1.使决策树拟合数据\n",
    "# 2.评估模型\n",
    "# 3.给不正确的样本增加权重。\n",
    "# 4.选择具有最大增量损失的叶子进行生长。\n",
    "# 5.Grow the tree。\n",
    "# 6.TO 2 评估模型\n",
    "\n",
    "# 我们需要设置两种基准。第一个将是naive预测。此预测将是基线得分，可与我们模型的得分进行比较，以评估我们是否有任何重大进展。\n",
    "# 在多类分类中，计算基线的最佳方法是假设每个类别的概率等于其在训练集中的平均频率。\n",
    "# 通过将每个类别的事件总和除以训练集的行数，可以轻松地计算频率。\n",
    "# 39类犯罪   每类预测的发生率 为数据集的频率\n",
    "\n",
    "naive_vals = train.groupby('Category').count().iloc[:,0]/train.shape[0]\n",
    "n_rows = test.shape[0]\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    np.repeat(np.array(naive_vals), n_rows).reshape(39, n_rows).transpose(),\n",
    "    columns=naive_vals.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用这种方法计算的基线为2.68015。 #https://www.kaggle.com/yannisp/sf-crime-analysis-prediction-naive-prediction\n",
    "# 我们可以注意到，该基线已经低于分类器的初始得分。\n",
    "# 另一个至关重要的基准通常是“人类性能”，可以作为贝叶斯错误率的替代指标。\n",
    "# 特定问题不属于人类擅长的领域（例如计算机视觉或NLP），\n",
    "# 因此，作为贝叶斯错误率的代理，我们将使用到目前为止最佳内核的分数，这是用户谢尔盖·列别捷夫（Sergey Lebedev）调整的，初始基准得分2.29318。\n",
    "# 基线得分和贝叶斯错误率之间的距离很小，这表明这是一个难题，改善幅度很小。\n",
    "\n",
    "#数据处理\n",
    "#按照问题陈述中描述的方法，我们确定了2323个重复值和67个错误的纬度。\n",
    "#删除重复项并估算离群值。\n",
    "#然后，我们创建了特征：\n",
    "#从“日期”字段中，我们提取了日期，月份，年份，小时，分钟，工作日以及从第一天开始的天数。\n",
    "#我们从“地址”字段中提取了事件是否发生在十字路口或建筑构件上。\n",
    "def feature_engineering(data):\n",
    "    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n",
    "    data['n_days'] = (\n",
    "        data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n",
    "    #从第一天开始的天数\n",
    "    data['Day'] = data['Dates'].dt.day\n",
    "    data['DayOfWeek'] = data['Dates'].dt.weekday\n",
    "    #工作日\n",
    "    data['Month'] = data['Dates'].dt.month\n",
    "    data['Year'] = data['Dates'].dt.year\n",
    "    data['Hour'] = data['Dates'].dt.hour\n",
    "    data['Minute'] = data['Dates'].dt.minute\n",
    "    data['Block'] = data['Address'].str.contains('block', case=False)\n",
    "    #“地址”字段中提取了事件是否发生在十字路口或建筑构件上---bool---True--False\n",
    "    data.drop(columns=['Dates','Date','Address'], inplace=True)\n",
    "        \n",
    "    return data\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train.drop(columns=['Descript','Resolution'], inplace=True)\n",
    "test = feature_engineering(test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征缩放\n",
    "# 决定继续使用基于树的算法，因此无需对最终数据集进行缩放。\n",
    "# 特征选择\n",
    "# 经过上述功能设计，我们最终获得了11个特征。 时间-地区-X-Y-建筑物地区\n",
    "#为了确定其中是否增加了模型的复杂性而不增加模型的可观收益，我们使用了Permutation Importance置换重要性方法。\n",
    "\n",
    "# 删一个特征看一次损失值\n",
    "# 想法是，可以通过查看功能不可用时损耗减少多少来衡量功能的重要性。\n",
    "# 为此，我们可以从数据集中删除每个特征，重新训练估计量并检查影响。\n",
    "# 这样做将需要重新训练每个功能的估计量，这可能需要大量计算。\n",
    "# 取而代之的是，我们可以通过特征的打乱后的值将其替换为噪声。？？？\n",
    "# 上述技术的实现表明，无需删除任何特征，因为所有特征都对数据集产生积极影响。\n",
    "\n",
    "#下例：删除PdDistrict 后用LGBMClassifier模型训练后得每个功能的估计量\n",
    "#即将离散型的数据转换成 0到n−1 之间的数，这里 n是一个列表的不同取值的个数\n",
    "#可以认为是某个特征的所有不同取值的个数。\n",
    "\n",
    "#编码\n",
    "le1 = LabelEncoder()\n",
    "train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n",
    "test['PdDistrict'] = le1.transform(test['PdDistrict'])\n",
    "le2 = LabelEncoder()\n",
    "y = le2.fit_transform(train.pop('Category'))\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train, y)\n",
    "\n",
    "model =LGBMClassifier(objective='multiclass', num_class=39).fit(train_X, train_y)\n",
    "\n",
    "perm = PermutationImportance(model).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names=val_X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立初始模型\n",
    "# 为了建立模型，我们使用了LightGBM的Python API。首先，我们通过组合特征和目标形成数据集，并将PdDistrict声明为分类变量来使用'lightgbm.Dataset（）。\n",
    "# 然后，我们将交叉验证与提前停止（10个回合）和参数一起使用：\n",
    "# Objective = 'multiclass',\n",
    "# 'Metric = ‘multi_logloss',\n",
    "# 'Num_class = 39\n",
    "#上面的设置在23个epoch后获得了2.46799的交叉验证得分，在测试集上获得了2.49136。\n",
    "\n",
    "\n",
    "#作为最终模型的结论，我们对100个时代使用了五重交叉验证，并通过贝叶斯优化提前停止。\n",
    "#此外，我们创建了一个自定义的回调函数，以便我们可以编写可由Tensorboard读取的正确日志。这样，我们可以实时监控验证过程。\n",
    "#以上在某种程度上是一个迭代过程。\n",
    "#我们运行优化过程，直到在Tensorboard中注意到模型收敛为止。然后，我们停止并评估了结果，然后移至下一个迭代。 （此处为流程示例）\n",
    "# 1.优化超参数\n",
    "# 2.模型收敛后进行下一轮微调  比上次多调了两个超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最终模型\n",
    "#See here\n",
    "\n",
    "#LoadData\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n",
    "\n",
    "# Data cleaning\n",
    "train.drop_duplicates(inplace=True)\n",
    "train.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "test.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "\n",
    "for district in train['PdDistrict'].unique():\n",
    "    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n",
    "        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n",
    "    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n",
    "        test.loc[test['PdDistrict'] == district, ['X', 'Y']])\n",
    "train_data = lgb.Dataset(\n",
    "    train, label=y, categorical_feature=['PdDistrict'], free_raw_data=False)\n",
    "\n",
    "# Feature Engineering\n",
    "train = feature_engineering(train)\n",
    "train.drop(columns=['Descript','Resolution'], inplace=True)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "# Encoding the Categorical Variables\n",
    "le1 = LabelEncoder()\n",
    "train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n",
    "test['PdDistrict'] = le1.transform(test['PdDistrict'])\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "X = train.drop(columns=['Category'])\n",
    "y= le2.fit_transform(train['Category'])\n",
    "\n",
    "# Creating the model---模型类---import lightgbm as lgb\n",
    "train_data = lgb.Dataset(\n",
    "    X, label=y, categorical_feature=['PdDistrict'])\n",
    "\n",
    "params = {'boosting':'gbdt',\n",
    "          'objective':'multiclass',\n",
    "          'num_class':39,\n",
    "          'max_delta_step':0.9,\n",
    "          'min_data_in_leaf': 21,\n",
    "          'learning_rate': 0.4,\n",
    "          'max_bin': 465,\n",
    "          'num_leaves': 41\n",
    "         }\n",
    "\n",
    "bst = lgb.train(params, train_data, 100)\n",
    "\n",
    "predictions = bst.predict(test)\n",
    "\n",
    "# Submitting the results\n",
    "submission = pd.DataFrame(\n",
    "    predictions,\n",
    "    columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')),\n",
    "    index=test.index)\n",
    "submission.to_csv(\n",
    "    'LGBM_final.csv', index_label='Id')\n",
    "\n",
    "\n",
    "#2.25697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#其实一个特征到底对模型影响如何呢\n",
    "\n",
    "#hour对赌博就没有影响 对别的犯罪类型就有点影响\n",
    "#from lightgbm import LGBMClassifier  ----分类器LGBMClassifier \n",
    "# Partial Dependencies 部份依赖图，39张图 横轴为hour 纵轴围殴hour得分布\n",
    "model = LGBMClassifier(**params).fit(X, y, categorical_feature=['PdDistrict'])\n",
    "\n",
    "pdp_Pd = pdp.pdp_isolate(\n",
    "    model=model,\n",
    "    dataset=X,\n",
    "    model_features=X.columns.tolist(),\n",
    "    feature='Hour',\n",
    "    n_jobs=-1)\n",
    "\n",
    "pdp.pdp_plot(\n",
    "    pdp_Pd,\n",
    "    'Hour',\n",
    "    ncols=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
